{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d6OSp7TTqfJ"
   },
   "source": [
    "이 자료는 위키독스 '딥 러닝을 이용한 자연어 처리 입문'의 트랜스포머 튜토리얼로 작성되었습니다.  \n",
    "2021년 10월 8일에 정상 동작을 확인하였습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3pVJtdaRZPe"
   },
   "source": [
    "챗봇의 성능이 아쉽다면 모델의 크기를 조정해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "In6C4s6xRqod"
   },
   "source": [
    "링크 :  \n",
    "https://wikidocs.net/31379  \n",
    "https://wikidocs.net/89786"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PgzTgkaL9OnI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "fvo8VpnvRcDM",
    "outputId": "6e81f94c-eb7c-42fd-bf23-7da2e10779c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JY6lwO-oWZJd"
   },
   "outputs": [],
   "source": [
    "# 최종 버전\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    angle_rads = np.zeros(angle_rads.shape)\n",
    "    angle_rads[:, 0::2] = sines\n",
    "    angle_rads[:, 1::2] = cosines\n",
    "    pos_encoding = tf.constant(angle_rads)\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "\n",
    "    print(pos_encoding.shape)\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "QiNrtgQUV36E",
    "outputId": "9723a13c-2b6c-401e-c929-d54af86c3ac5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_pos_encoding = PositionalEncoding(50, 128)\n",
    "\n",
    "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 128))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "F6534luH9d0K"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "  # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "  # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "  # padding_mask : (batch_size, 1, 1, key의 문장 길이)\n",
    "\n",
    "  # Q와 K의 곱. 어텐션 스코어 행렬.\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 스케일링\n",
    "  # dk의 루트값으로 나눠준다.\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.\n",
    "  # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\n",
    "  # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RNU5ilcw9f3q"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    # d_model을 num_heads로 나눈 값.\n",
    "    # 논문 기준 : 64\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    # WQ, WK, WV에 해당하는 밀집층 정의\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    # WO에 해당하는 밀집층 정의\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  # num_heads 개수만큼 q, k, v를 split하는 함수\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n",
    "    # q : (batch_size, query의 문장 길이, d_model)\n",
    "    # k : (batch_size, key의 문장 길이, d_model)\n",
    "    # v : (batch_size, value의 문장 길이, d_model)\n",
    "    # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # 2. 헤드 나누기\n",
    "    # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "    # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n",
    "    # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "    # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 4. 헤드 연결(concatenate)하기\n",
    "    # (batch_size, query의 문장 길이, d_model)\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 5. WO에 해당하는 밀집층 지나기\n",
    "    # (batch_size, query의 문장 길이, d_model)\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QrCdqp8z9ily"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, key의 문장 길이)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nskamzrs9j1K"
   },
   "outputs": [],
   "source": [
    "def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # 인코더는 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "          'mask': padding_mask # 패딩 마스크 사용\n",
    "      })\n",
    "\n",
    "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)\n",
    "  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "34a0WK-B9lgi"
   },
   "outputs": [],
   "source": [
    "# @@@@수정됨\n",
    "def encoder(vocab_size, num_layers, dff,\n",
    "            d_model, num_heads, dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 인코더는 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 포지셔널 인코딩 + 드롭아웃\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # 인코더를 num_layers개 쌓기\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "        dropout=dropout, name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ng1ysGoJ9mha"
   },
   "outputs": [],
   "source": [
    "# 디코더의 첫번째 서브층(sublayer)에서 미래 토큰을 Mask하는 함수\n",
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x) # 패딩 마스크도 포함\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "I3QDoLz49oPK"
   },
   "outputs": [],
   "source": [
    "def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "\n",
    "  # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "          'mask': look_ahead_mask # 룩어헤드 마스크\n",
    "      })\n",
    "\n",
    "  # 잔차 연결과 층 정규화\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V\n",
    "          'mask': padding_mask # 패딩 마스크\n",
    "      })\n",
    "\n",
    "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\n",
    "  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5i9lZmji9qDa"
   },
   "outputs": [],
   "source": [
    "def decoder(vocab_size, num_layers, dff,\n",
    "            d_model, num_heads, dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "\n",
    "  # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 포지셔널 인코딩 + 드롭아웃\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # 디코더를 num_layers개 쌓기\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "        dropout=dropout, name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-GEC6CXF9q7S"
   },
   "outputs": [],
   "source": [
    "def transformer(vocab_size, num_layers, dff,\n",
    "                d_model, num_heads, dropout,\n",
    "                name=\"transformer\"):\n",
    "\n",
    "  # 인코더의 입력\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 디코더의 입력\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더의 패딩 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더의 룩어헤드 마스크(첫번째 서브층)\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask, output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 디코더의 패딩 마스크(두번째 서브층)\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더의 출력은 enc_outputs. 디코더로 전달된다.\n",
    "  enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "      d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크\n",
    "\n",
    "  # 디코더의 출력은 dec_outputs. 출력층으로 전달된다.\n",
    "  dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "      d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 다음 단어 예측을 위한 출력층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "ori23vQy9sRq",
    "outputId": "0ef89729-18b1-4316-baf3-002bc36d8400"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9000, 128)\n",
      "(1, 9000, 128)\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "small_transformer = transformer(\n",
    "    vocab_size = 9000,\n",
    "    num_layers = 4,\n",
    "    dff = 512,\n",
    "    d_model = 128,\n",
    "    num_heads = 4,\n",
    "    dropout = 0.3,\n",
    "    name=\"small_transformer\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    small_transformer, to_file='small_transformer.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7DgCW2_k-8KM"
   },
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kkkz7qka_tSU"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "k4lA_Efe_uOx",
    "outputId": "7aeca28a-3fd1-4f89-9eca-a999caeddc5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9tpZXei_1xK"
   },
   "source": [
    "# 여기서부터 챗봇 개발"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "M5W2D7Uq_u_x"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "t7ku6KRb_545",
    "outputId": "a13612b3-3fc3-431b-fc82-9de17840bef0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q</th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>코로나 검사결과는 언제 나와요?</td>\n",
       "      <td>6시간에서 하루 정도 소요됩니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>결과는 어떻게 알 수 있어요?</td>\n",
       "      <td>문자로 양성, 음성 여부를 알려줍니다. .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>하루가 지나도 연락이 안오면 어떻게 해요?</td>\n",
       "      <td>검사 받으신 진료소로 전화주셔서 확인하시면 됩니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>친구 아버지가 돌아가셨는데 장례식장에 가도 돼요?</td>\n",
       "      <td>가셔도 되지만, 가급적 사람들이 많이 모이는 곳은 피하시는 것이 좋습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>장례식장에 마스크 안끼고 가도 돼요?</td>\n",
       "      <td>마스크는 꼭 착용하셔야 합니다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              q                                           a\n",
       "0            코로나 검사결과는 언제 나와요?                          6시간에서 하루 정도 소요됩니다. \n",
       "1             결과는 어떻게 알 수 있어요?                     문자로 양성, 음성 여부를 알려줍니다. . \n",
       "2       하루가 지나도 연락이 안오면 어떻게 해요?               검사 받으신 진료소로 전화주셔서 확인하시면 됩니다. \n",
       "3  친구 아버지가 돌아가셨는데 장례식장에 가도 돼요?   가셔도 되지만, 가급적 사람들이 많이 모이는 곳은 피하시는 것이 좋습니다. \n",
       "4          장례식장에 마스크 안끼고 가도 돼요?                          마스크는 꼭 착용하셔야 합니다. "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('intetraining2.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3hpk8Z-P_6hh",
    "outputId": "8f49c5d4-827a-4ea5-8f36-3409803e8832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "챗봇 샘플의 개수 : 7451\n"
     ]
    }
   ],
   "source": [
    "print('챗봇 샘플의 개수 :', len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bXg6ws2s_9Ch",
    "outputId": "c1cc75b1-b786-486c-c443-2135c96bdda0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q    0\n",
      "a    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "9AKOHALL_9rJ"
   },
   "outputs": [],
   "source": [
    "questions = []\n",
    "for sentence in train_data['q']:\n",
    "    # 구두점에 대해서 띄어쓰기\n",
    "    # ex) 12시 땡! -> 12시 땡 !\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    questions.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "GoJs0Od8_-aB"
   },
   "outputs": [],
   "source": [
    "answers = []\n",
    "for sentence in train_data['a']:\n",
    "    # 구두점에 대해서 띄어쓰기\n",
    "    # ex) 12시 땡! -> 12시 땡 !\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    answers.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6AS2olsI__KJ",
    "outputId": "241a657a-aa22-4549-fc36-65ec00c0c1ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7451"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KM7QxzSG__9B",
    "outputId": "25909c69-0928-4be6-83f5-0c5b4de7603b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코로나 검사결과는 언제 나와요 ?', '결과는 어떻게 알 수 있어요 ?', '하루가 지나도 연락이 안오면 어떻게 해요 ?', '친구 아버지가 돌아가셨는데 장례식장에 가도 돼요 ?', '장례식장에 마스크 안끼고 가도 돼요 ?']\n",
      "['6시간에서 하루 정도 소요됩니다 .', '문자로 양성 ,  음성 여부를 알려줍니다 .   .', '검사 받으신 진료소로 전화주셔서 확인하시면 됩니다 .', '가셔도 되지만 ,  가급적 사람들이 많이 모이는 곳은 피하시는 것이 좋습니다 .', '마스크는 꼭 착용하셔야 합니다 .']\n"
     ]
    }
   ],
   "source": [
    "print(questions[:5])\n",
    "print(answers[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "iSYgYFMVABRq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team and Jangwon Park\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" Tokenization classes for KoBERT model \"\"\"\n",
    "\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import unicodedata\n",
    "from shutil import copyfile\n",
    "\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "VOCAB_FILES_NAMES = {\n",
    "    \"vocab_file\": \"tokenizer_78b3253a26.model\",\n",
    "    \"vocab_txt\": \"vocab.txt\",\n",
    "}\n",
    "\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\n",
    "    \"vocab_file\": {\n",
    "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
    "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
    "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\",\n",
    "    },\n",
    "    \"vocab_txt\": {\n",
    "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
    "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
    "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\",\n",
    "    },\n",
    "}\n",
    "\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "    \"monologg/kobert\": 512,\n",
    "    \"monologg/kobert-lm\": 512,\n",
    "    \"monologg/distilkobert\": 512,\n",
    "}\n",
    "\n",
    "PRETRAINED_INIT_CONFIGURATION = {\n",
    "    \"monologg/kobert\": {\"do_lower_case\": False},\n",
    "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n",
    "    \"monologg/distilkobert\": {\"do_lower_case\": False},\n",
    "}\n",
    "\n",
    "SPIECE_UNDERLINE = \"▁\"\n",
    "\n",
    "\n",
    "class KoBertTokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"\n",
    "    SentencePiece based tokenizer. Peculiarities:\n",
    "        - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        vocab_txt,\n",
    "        do_lower_case=False,\n",
    "        remove_space=True,\n",
    "        keep_accents=False,\n",
    "        unk_token=\"[UNK]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            pad_token=pad_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Build vocab\n",
    "        self.token2idx = dict()\n",
    "        self.idx2token = []\n",
    "        with open(vocab_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "            for idx, token in enumerate(f):\n",
    "                token = token.strip()\n",
    "                self.token2idx[token] = idx\n",
    "                self.idx2token.append(token)\n",
    "\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "        except ImportError:\n",
    "            logger.warning(\n",
    "                \"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
    "                \"pip install sentencepiece\"\n",
    "            )\n",
    "\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.remove_space = remove_space\n",
    "        self.keep_accents = keep_accents\n",
    "        self.vocab_file = vocab_file\n",
    "        self.vocab_txt = vocab_txt\n",
    "\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.Load(vocab_file)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.idx2token)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return dict(self.token2idx, **self.added_tokens_encoder)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state[\"sp_model\"] = None\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        self.__dict__ = d\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "        except ImportError:\n",
    "            logger.warning(\n",
    "                \"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
    "                \"pip install sentencepiece\"\n",
    "            )\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.Load(self.vocab_file)\n",
    "\n",
    "    def preprocess_text(self, inputs):\n",
    "        if self.remove_space:\n",
    "            outputs = \" \".join(inputs.strip().split())\n",
    "        else:\n",
    "            outputs = inputs\n",
    "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
    "\n",
    "        if not self.keep_accents:\n",
    "            outputs = unicodedata.normalize(\"NFKD\", outputs)\n",
    "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
    "        if self.do_lower_case:\n",
    "            outputs = outputs.lower()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        \"\"\"Tokenize a string.\"\"\"\n",
    "        text = self.preprocess_text(text)\n",
    "        pieces = self.sp_model.encode(text, out_type=str)\n",
    "        new_pieces = []\n",
    "        for piece in pieces:\n",
    "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
    "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n",
    "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
    "                    if len(cur_pieces[0]) == 1:\n",
    "                        cur_pieces = cur_pieces[1:]\n",
    "                    else:\n",
    "                        cur_pieces[0] = cur_pieces[0][1:]\n",
    "                cur_pieces.append(piece[-1])\n",
    "                new_pieces.extend(cur_pieces)\n",
    "            else:\n",
    "                new_pieces.append(piece)\n",
    "\n",
    "        return new_pieces\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
    "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
    "        return self.idx2token[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n",
    "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n",
    "        return out_string\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "        by concatenating and adding special tokens.\n",
    "        A KoBERT sequence has the following format:\n",
    "            single sequence: [CLS] X [SEP]\n",
    "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
    "\n",
    "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
    "        Args:\n",
    "            token_ids_0: list of ids (must not contain special tokens)\n",
    "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
    "                for sequence pairs\n",
    "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
    "                special tokens for the model\n",
    "        Returns:\n",
    "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
    "        \"\"\"\n",
    "\n",
    "        if already_has_special_tokens:\n",
    "            if token_ids_1 is not None:\n",
    "                raise ValueError(\n",
    "                    \"You should not supply a second sequence if the provided sequence of \"\n",
    "                    \"ids is already formated with special tokens for the model.\"\n",
    "                )\n",
    "            return list(\n",
    "                map(\n",
    "                    lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0,\n",
    "                    token_ids_0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "\n",
    "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
    "        A KoBERT sequence pair mask has the following format:\n",
    "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence\n",
    "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
    "        \"\"\"\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        if token_ids_1 is None:\n",
    "            return len(cls + token_ids_0 + sep) * [0]\n",
    "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
    "\n",
    "    def save_vocabulary(self, save_directory):\n",
    "        \"\"\"Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
    "        to a directory.\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(save_directory):\n",
    "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
    "            return\n",
    "\n",
    "        # 1. Save sentencepiece model\n",
    "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
    "\n",
    "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n",
    "            copyfile(self.vocab_file, out_vocab_model)\n",
    "\n",
    "        # 2. Save vocab.txt\n",
    "        index = 0\n",
    "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n",
    "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    logger.warning(\n",
    "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
    "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(token + \"\\n\")\n",
    "                index += 1\n",
    "\n",
    "        return out_vocab_model, out_vocab_txt\n",
    "\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\n",
    "\n",
    "# 시작 토큰과 종료 토큰에 대한 정수 부여.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "# 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ln_by7YmGPTI",
    "outputId": "956368b6-ada2-4bc0-8321-8696f4b3c808"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시작 토큰 번호 : [8002]\n",
      "종료 토큰 번호 : [8003]\n",
      "단어 집합의 크기 : 8004\n"
     ]
    }
   ],
   "source": [
    "print('시작 토큰 번호 :',START_TOKEN)\n",
    "print('종료 토큰 번호 :',END_TOKEN)\n",
    "print('단어 집합의 크기 :',VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OnCM12LCACDB",
    "outputId": "2548a1f3-f7a4-405f-ad23-950a36fbfa43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sample question: [2, 1202, 3123, 7086, 1907, 6682, 4424, 7789, 5212, 5850, 6999, 3]\n"
     ]
    }
   ],
   "source": [
    "# 서브워드텍스트인코더 토크나이저의 .encode()를 사용하여 텍스트 시퀀스를 정수 시퀀스로 변환.\n",
    "print('Tokenized sample question: {}'.format(tokenizer.encode(questions[20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6616jr1ALcS",
    "outputId": "6c88cf55-4a2d-4ccc-8f33-7be94fe2bba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 [2, 1202, 3123, 7086, 1907, 6682, 4424, 7789, 5212, 5850, 6999, 3]\n",
      "기존 문장: [CLS] 그런데 아이들은 마스크 착용하기 힘든데요[SEP]\n"
     ]
    }
   ],
   "source": [
    "# 서브워드텍스트인코더 토크나이저의 .encode()와 decode() 테스트해보기\n",
    "\n",
    "# 임의의 입력 문장을 sample_string에 저장\n",
    "sample_string = questions[20]\n",
    "\n",
    "# encode() : 텍스트 시퀀스 --> 정수 시퀀스\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
    "\n",
    "# decode() : 정수 시퀀스 --> 텍스트 시퀀스\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장: {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ySXbL7GQAPhS",
    "outputId": "3cd817e4-2b2d-412b-e433-a63a120ab248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ----> [CLS]\n",
      "1202 ----> 그런데\n",
      "3123 ----> 아이들\n",
      "7086 ----> 은\n",
      "1907 ----> 마\n",
      "6682 ----> 스크\n",
      "4424 ----> 착용\n",
      "7789 ----> 하기\n",
      "5212 ----> 힘든\n",
      "5850 ----> 데\n",
      "6999 ----> 요\n",
      "3 ----> [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 각 정수는 각 단어와 어떻게 mapping되는지 병렬로 출력\n",
    "# 서브워드텍스트인코더는 의미있는 단위의 서브워드로 토크나이징한다. 띄어쓰기 단위 X 형태소 분석 단위 X\n",
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Dl69HqzjATYG"
   },
   "outputs": [],
   "source": [
    "# 최대 길이를 40으로 정의\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    tokenized_inputs.append(sentence1)\n",
    "    tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "qJJc_H5oAWmC"
   },
   "outputs": [],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-iBwtZqAXxC",
    "outputId": "e613b0e5-5cdb-4d70-8e36-ed5fdbcbd4d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 데이터의 크기(shape) : (7451, 40)\n",
      "답변 데이터의 크기(shape) : (7451, 40)\n"
     ]
    }
   ],
   "source": [
    "print('질문 데이터의 크기(shape) :', questions.shape)\n",
    "print('답변 데이터의 크기(shape) :', answers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fUnwWj4pAYox",
    "outputId": "585b0cf1-85bc-49a3-d8c0-0d1b0a851436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8002    2 4665 6079 5655  897 5416 5760 3245 1394 6999  633    3 8003\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "[8002    2  617 6706 6903 4937 4099 2843 5906  517   54    3 8003    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# 0번째 샘플을 임의로 출력\n",
    "print(questions[0])\n",
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ThFD6K_tAZN5",
    "outputId": "62409b7a-a456-4c8c-ce2a-4ee819b8576b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기(Vocab size): 8004\n",
      "전체 샘플의 수(Number of samples): 7451\n"
     ]
    }
   ],
   "source": [
    "print('단어 집합의 크기(Vocab size): {}'.format(VOCAB_SIZE))\n",
    "print('전체 샘플의 수(Number of samples): {}'.format(len(questions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_FXoh56KAaBx"
   },
   "outputs": [],
   "source": [
    "# 텐서플로우 dataset을 이용하여 셔플(shuffle)을 수행하되, 배치 크기로 데이터를 묶는다.\n",
    "# 또한 이 과정에서 교사 강요(teacher forcing)을 사용하기 위해서 디코더의 입력과 실제값 시퀀스를 구성한다.\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더의 실제값 시퀀스에서는 시작 토큰을 제거해야 한다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1] # 디코더의 입력. 마지막 패딩 토큰이 제거된다.\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]  # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다.\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H9vu74odAaxB",
    "outputId": "a45ba996-415a-4e39-b40c-40d34bfa0d23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8002    2  617 6706 6903 4937 4099 2843 5906  517   54    3 8003    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "[[8002    2  617 6706 6903 4937 4099 2843 5906  517   54    3 8003    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]]\n",
      "[[   2  617 6706 6903 4937 4099 2843 5906  517   54    3 8003    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 샘플에 대해서 [:, :-1]과 [:, 1:]이 어떤 의미를 가지는지 테스트해본다.\n",
    "print(answers[0]) # 기존 샘플\n",
    "print(answers[:1][:, :-1]) # 마지막 패딩 토큰 제거하면서 길이가 39가 된다.\n",
    "print(answers[:1][:, 1:]) # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다. 길이는 역시 39가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0tcomE6AblR",
    "outputId": "e1e24873-dd34-4227-90cd-dba343f2eb76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8004, 256)\n",
      "(1, 8004, 256)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Hyper-parameters\n",
    "NUM_LAYERS = 2\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "DFF = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dff=DFF,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "w3qpxTy7Ainq"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "jq-Ow4L0Aoqy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "117/117 [==============================] - 16s 90ms/step - loss: 3.9957 - accuracy: 0.0211\n",
      "Epoch 2/1000\n",
      "117/117 [==============================] - 11s 90ms/step - loss: 3.3897 - accuracy: 0.0736\n",
      "Epoch 3/1000\n",
      "117/117 [==============================] - 11s 93ms/step - loss: 2.6596 - accuracy: 0.1279\n",
      "Epoch 4/1000\n",
      "117/117 [==============================] - 11s 95ms/step - loss: 2.2159 - accuracy: 0.1439\n",
      "Epoch 5/1000\n",
      "117/117 [==============================] - 11s 96ms/step - loss: 1.8948 - accuracy: 0.1837\n",
      "Epoch 6/1000\n",
      "117/117 [==============================] - 12s 99ms/step - loss: 1.6348 - accuracy: 0.2076\n",
      "Epoch 7/1000\n",
      "117/117 [==============================] - 12s 100ms/step - loss: 1.4342 - accuracy: 0.2283\n",
      "Epoch 8/1000\n",
      "117/117 [==============================] - 12s 100ms/step - loss: 1.2658 - accuracy: 0.2467s - loss: 1.2664 - accuracy: \n",
      "Epoch 9/1000\n",
      "117/117 [==============================] - 12s 101ms/step - loss: 1.1174 - accuracy: 0.2634\n",
      "Epoch 10/1000\n",
      "117/117 [==============================] - 12s 101ms/step - loss: 0.9838 - accuracy: 0.2809\n",
      "Epoch 11/1000\n",
      "117/117 [==============================] - 12s 102ms/step - loss: 0.8647 - accuracy: 0.2975\n",
      "Epoch 12/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.7598 - accuracy: 0.3123\n",
      "Epoch 13/1000\n",
      "117/117 [==============================] - 12s 104ms/step - loss: 0.6663 - accuracy: 0.3270\n",
      "Epoch 14/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.5822 - accuracy: 0.3406s - loss: 0.5816 - accura\n",
      "Epoch 15/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.5057 - accuracy: 0.3530\n",
      "Epoch 16/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.4385 - accuracy: 0.3648\n",
      "Epoch 17/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.3783 - accuracy: 0.3752s - l\n",
      "Epoch 18/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.3272 - accuracy: 0.3846\n",
      "Epoch 19/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.2808 - accuracy: 0.3926\n",
      "Epoch 20/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.2444 - accuracy: 0.3990s - loss: 0.2437 - accuracy\n",
      "Epoch 21/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.2125 - accuracy: 0.4060\n",
      "Epoch 22/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.1850 - accuracy: 0.4122\n",
      "Epoch 23/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.1657 - accuracy: 0.4158\n",
      "Epoch 24/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.1488 - accuracy: 0.4198\n",
      "Epoch 25/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.1349 - accuracy: 0.4231s - l\n",
      "Epoch 26/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.1283 - accuracy: 0.4244\n",
      "Epoch 27/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.1198 - accuracy: 0.4265\n",
      "Epoch 28/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.1167 - accuracy: 0.4273\n",
      "Epoch 29/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.1121 - accuracy: 0.4283\n",
      "Epoch 30/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.1092 - accuracy: 0.4288\n",
      "Epoch 31/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.1065 - accuracy: 0.4298\n",
      "Epoch 32/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.1035 - accuracy: 0.4305\n",
      "Epoch 33/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.1041 - accuracy: 0.4302\n",
      "Epoch 34/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.1016 - accuracy: 0.4313s - loss: 0.1009 \n",
      "Epoch 35/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.1011 - accuracy: 0.4310\n",
      "Epoch 36/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0955 - accuracy: 0.4322\n",
      "Epoch 37/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0892 - accuracy: 0.4342\n",
      "Epoch 38/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0853 - accuracy: 0.4354\n",
      "Epoch 39/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0822 - accuracy: 0.4363\n",
      "Epoch 40/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0778 - accuracy: 0.4373\n",
      "Epoch 41/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0756 - accuracy: 0.4383\n",
      "Epoch 42/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0715 - accuracy: 0.4391\n",
      "Epoch 43/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0687 - accuracy: 0.4397\n",
      "Epoch 44/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0664 - accuracy: 0.4404\n",
      "Epoch 45/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0630 - accuracy: 0.4413\n",
      "Epoch 46/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0625 - accuracy: 0.4417\n",
      "Epoch 47/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0608 - accuracy: 0.4423\n",
      "Epoch 48/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0585 - accuracy: 0.4426\n",
      "Epoch 49/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0564 - accuracy: 0.4435\n",
      "Epoch 50/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0550 - accuracy: 0.4438\n",
      "Epoch 51/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0549 - accuracy: 0.4440\n",
      "Epoch 52/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0510 - accuracy: 0.4451\n",
      "Epoch 53/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0501 - accuracy: 0.4452\n",
      "Epoch 54/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0487 - accuracy: 0.4457\n",
      "Epoch 55/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0481 - accuracy: 0.4459\n",
      "Epoch 56/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0467 - accuracy: 0.4460\n",
      "Epoch 57/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0458 - accuracy: 0.4464\n",
      "Epoch 58/1000\n",
      "117/117 [==============================] - 12s 103ms/step - loss: 0.0445 - accuracy: 0.4467s - loss:\n",
      "Epoch 59/1000\n",
      "117/117 [==============================] - 13s 115ms/step - loss: 0.0431 - accuracy: 0.4471\n",
      "Epoch 60/1000\n",
      "117/117 [==============================] - 14s 124ms/step - loss: 0.0434 - accuracy: 0.4471\n",
      "Epoch 61/1000\n",
      "117/117 [==============================] - 15s 126ms/step - loss: 0.0426 - accuracy: 0.4472\n",
      "Epoch 62/1000\n",
      "117/117 [==============================] - 14s 117ms/step - loss: 0.0408 - accuracy: 0.4477\n",
      "Epoch 63/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0398 - accuracy: 0.4479\n",
      "Epoch 64/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0397 - accuracy: 0.4481\n",
      "Epoch 65/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0379 - accuracy: 0.4486\n",
      "Epoch 66/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0375 - accuracy: 0.4488\n",
      "Epoch 67/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0378 - accuracy: 0.4490\n",
      "Epoch 68/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0367 - accuracy: 0.4489\n",
      "Epoch 69/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0359 - accuracy: 0.4491\n",
      "Epoch 70/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0354 - accuracy: 0.4494\n",
      "Epoch 71/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0351 - accuracy: 0.4495\n",
      "Epoch 72/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0349 - accuracy: 0.4494\n",
      "Epoch 73/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0327 - accuracy: 0.4501\n",
      "Epoch 74/1000\n",
      "117/117 [==============================] - 13s 108ms/step - loss: 0.0333 - accuracy: 0.4499\n",
      "Epoch 75/1000\n",
      "117/117 [==============================] - 13s 108ms/step - loss: 0.0328 - accuracy: 0.4499\n",
      "Epoch 76/1000\n",
      "117/117 [==============================] - 13s 108ms/step - loss: 0.0329 - accuracy: 0.4499\n",
      "Epoch 77/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 13s 108ms/step - loss: 0.0317 - accuracy: 0.4503\n",
      "Epoch 78/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0313 - accuracy: 0.4503\n",
      "Epoch 79/1000\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0309 - accuracy: 0.4504\n",
      "Epoch 80/1000\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0312 - accuracy: 0.4503\n",
      "Epoch 81/1000\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0300 - accuracy: 0.4506\n",
      "Epoch 82/1000\n",
      "117/117 [==============================] - 13s 114ms/step - loss: 0.0299 - accuracy: 0.4507\n",
      "Epoch 83/1000\n",
      "117/117 [==============================] - 14s 116ms/step - loss: 0.0293 - accuracy: 0.4505\n",
      "Epoch 84/1000\n",
      "117/117 [==============================] - 14s 116ms/step - loss: 0.0294 - accuracy: 0.4509\n",
      "Epoch 85/1000\n",
      "117/117 [==============================] - 14s 116ms/step - loss: 0.0288 - accuracy: 0.4510\n",
      "Epoch 86/1000\n",
      "117/117 [==============================] - 14s 117ms/step - loss: 0.0288 - accuracy: 0.4509\n",
      "Epoch 87/1000\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0281 - accuracy: 0.4510\n",
      "Epoch 88/1000\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0281 - accuracy: 0.4510\n",
      "Epoch 89/1000\n",
      "117/117 [==============================] - 14s 116ms/step - loss: 0.0280 - accuracy: 0.4512\n",
      "Epoch 90/1000\n",
      "117/117 [==============================] - 14s 116ms/step - loss: 0.0270 - accuracy: 0.4513\n",
      "Epoch 91/1000\n",
      "117/117 [==============================] - 13s 112ms/step - loss: 0.0270 - accuracy: 0.4514\n",
      "Epoch 92/1000\n",
      "117/117 [==============================] - 14s 116ms/step - loss: 0.0270 - accuracy: 0.4514\n",
      "Epoch 93/1000\n",
      "117/117 [==============================] - 15s 124ms/step - loss: 0.0271 - accuracy: 0.4514\n",
      "Epoch 94/1000\n",
      "117/117 [==============================] - 14s 120ms/step - loss: 0.0263 - accuracy: 0.4515\n",
      "Epoch 95/1000\n",
      "117/117 [==============================] - 14s 120ms/step - loss: 0.0260 - accuracy: 0.4516\n",
      "Epoch 96/1000\n",
      "117/117 [==============================] - 14s 120ms/step - loss: 0.0256 - accuracy: 0.4517\n",
      "Epoch 97/1000\n",
      "117/117 [==============================] - 14s 120ms/step - loss: 0.0256 - accuracy: 0.4519\n",
      "Epoch 98/1000\n",
      "117/117 [==============================] - 14s 118ms/step - loss: 0.0255 - accuracy: 0.4518\n",
      "Epoch 99/1000\n",
      "117/117 [==============================] - 14s 123ms/step - loss: 0.0255 - accuracy: 0.4518\n",
      "Epoch 100/1000\n",
      "117/117 [==============================] - 14s 121ms/step - loss: 0.0250 - accuracy: 0.4519\n",
      "Epoch 101/1000\n",
      "117/117 [==============================] - 14s 121ms/step - loss: 0.0245 - accuracy: 0.4518\n",
      "Epoch 102/1000\n",
      "117/117 [==============================] - 14s 120ms/step - loss: 0.0242 - accuracy: 0.4520\n",
      "Epoch 103/1000\n",
      "117/117 [==============================] - 14s 122ms/step - loss: 0.0246 - accuracy: 0.4519\n",
      "Epoch 104/1000\n",
      "117/117 [==============================] - 14s 119ms/step - loss: 0.0238 - accuracy: 0.4520\n",
      "Epoch 105/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0242 - accuracy: 0.4520\n",
      "Epoch 106/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0237 - accuracy: 0.4521\n",
      "Epoch 107/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0239 - accuracy: 0.4521\n",
      "Epoch 108/1000\n",
      "117/117 [==============================] - 13s 112ms/step - loss: 0.0236 - accuracy: 0.4521\n",
      "Epoch 109/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0234 - accuracy: 0.4522\n",
      "Epoch 110/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0226 - accuracy: 0.4526\n",
      "Epoch 111/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0229 - accuracy: 0.4524\n",
      "Epoch 112/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0228 - accuracy: 0.4524\n",
      "Epoch 113/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0231 - accuracy: 0.4523\n",
      "Epoch 114/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0226 - accuracy: 0.4523\n",
      "Epoch 115/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0221 - accuracy: 0.4525\n",
      "Epoch 116/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0220 - accuracy: 0.4525\n",
      "Epoch 117/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0219 - accuracy: 0.4527\n",
      "Epoch 118/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0219 - accuracy: 0.4526\n",
      "Epoch 119/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0216 - accuracy: 0.4526\n",
      "Epoch 120/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0220 - accuracy: 0.4526\n",
      "Epoch 121/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0216 - accuracy: 0.4528\n",
      "Epoch 122/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0217 - accuracy: 0.4527\n",
      "Epoch 123/1000\n",
      "117/117 [==============================] - 14s 116ms/step - loss: 0.0214 - accuracy: 0.4526\n",
      "Epoch 124/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0213 - accuracy: 0.4527\n",
      "Epoch 125/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0204 - accuracy: 0.4528\n",
      "Epoch 126/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0209 - accuracy: 0.4528\n",
      "Epoch 127/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0211 - accuracy: 0.4527\n",
      "Epoch 128/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0205 - accuracy: 0.4529\n",
      "Epoch 129/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0209 - accuracy: 0.4528\n",
      "Epoch 130/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0209 - accuracy: 0.4529\n",
      "Epoch 131/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0209 - accuracy: 0.4526\n",
      "Epoch 132/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0200 - accuracy: 0.4530\n",
      "Epoch 133/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0204 - accuracy: 0.4527\n",
      "Epoch 134/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0201 - accuracy: 0.4531\n",
      "Epoch 135/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0202 - accuracy: 0.4529\n",
      "Epoch 136/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0195 - accuracy: 0.4531\n",
      "Epoch 137/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0200 - accuracy: 0.4530\n",
      "Epoch 138/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0194 - accuracy: 0.4531\n",
      "Epoch 139/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0198 - accuracy: 0.4530\n",
      "Epoch 140/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0198 - accuracy: 0.4532\n",
      "Epoch 141/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0195 - accuracy: 0.4528\n",
      "Epoch 142/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0194 - accuracy: 0.4532\n",
      "Epoch 143/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0197 - accuracy: 0.4529\n",
      "Epoch 144/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0193 - accuracy: 0.4532\n",
      "Epoch 145/1000\n",
      "117/117 [==============================] - 13s 112ms/step - loss: 0.0196 - accuracy: 0.4529\n",
      "Epoch 146/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0191 - accuracy: 0.4532\n",
      "Epoch 147/1000\n",
      "117/117 [==============================] - 14s 118ms/step - loss: 0.0192 - accuracy: 0.4532\n",
      "Epoch 148/1000\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0192 - accuracy: 0.4530\n",
      "Epoch 149/1000\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0188 - accuracy: 0.4532\n",
      "Epoch 150/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0189 - accuracy: 0.4532\n",
      "Epoch 151/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0186 - accuracy: 0.4535\n",
      "Epoch 152/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0186 - accuracy: 0.4533\n",
      "Epoch 153/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0187 - accuracy: 0.4533\n",
      "Epoch 154/1000\n",
      "117/117 [==============================] - 14s 117ms/step - loss: 0.0184 - accuracy: 0.4534\n",
      "Epoch 155/1000\n",
      "117/117 [==============================] - 14s 116ms/step - loss: 0.0184 - accuracy: 0.4535\n",
      "Epoch 156/1000\n",
      "117/117 [==============================] - 14s 118ms/step - loss: 0.0184 - accuracy: 0.4532\n",
      "Epoch 157/1000\n",
      "117/117 [==============================] - 14s 117ms/step - loss: 0.0183 - accuracy: 0.4533\n",
      "Epoch 158/1000\n",
      "117/117 [==============================] - 15s 126ms/step - loss: 0.0185 - accuracy: 0.4533\n",
      "Epoch 159/1000\n",
      "117/117 [==============================] - 14s 118ms/step - loss: 0.0183 - accuracy: 0.4533\n",
      "Epoch 160/1000\n",
      "117/117 [==============================] - 14s 118ms/step - loss: 0.0183 - accuracy: 0.4534\n",
      "Epoch 161/1000\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0183 - accuracy: 0.4534\n",
      "Epoch 162/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0181 - accuracy: 0.4533\n",
      "Epoch 163/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0179 - accuracy: 0.4533\n",
      "Epoch 164/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0174 - accuracy: 0.4537\n",
      "Epoch 165/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0182 - accuracy: 0.4533\n",
      "Epoch 166/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0179 - accuracy: 0.4535\n",
      "Epoch 167/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0177 - accuracy: 0.4535\n",
      "Epoch 168/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0177 - accuracy: 0.4534\n",
      "Epoch 169/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0181 - accuracy: 0.4533\n",
      "Epoch 170/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0176 - accuracy: 0.4534\n",
      "Epoch 171/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0174 - accuracy: 0.4535\n",
      "Epoch 172/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0176 - accuracy: 0.4535\n",
      "Epoch 173/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0175 - accuracy: 0.4536\n",
      "Epoch 174/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0175 - accuracy: 0.4536\n",
      "Epoch 175/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0173 - accuracy: 0.4536\n",
      "Epoch 176/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0173 - accuracy: 0.4536\n",
      "Epoch 177/1000\n",
      "117/117 [==============================] - 14s 120ms/step - loss: 0.0171 - accuracy: 0.4536\n",
      "Epoch 178/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0175 - accuracy: 0.4535\n",
      "Epoch 179/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0172 - accuracy: 0.4536\n",
      "Epoch 180/1000\n",
      "117/117 [==============================] - 13s 115ms/step - loss: 0.0172 - accuracy: 0.4536\n",
      "Epoch 181/1000\n",
      "117/117 [==============================] - 13s 112ms/step - loss: 0.0176 - accuracy: 0.4535\n",
      "Epoch 182/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0166 - accuracy: 0.4537\n",
      "Epoch 183/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0176 - accuracy: 0.4535\n",
      "Epoch 184/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0171 - accuracy: 0.4536\n",
      "Epoch 185/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0166 - accuracy: 0.4538\n",
      "Epoch 186/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0169 - accuracy: 0.4537\n",
      "Epoch 187/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0171 - accuracy: 0.4536\n",
      "Epoch 188/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0167 - accuracy: 0.4537\n",
      "Epoch 189/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0165 - accuracy: 0.4537\n",
      "Epoch 190/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0169 - accuracy: 0.4535\n",
      "Epoch 191/1000\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0167 - accuracy: 0.4537\n",
      "Epoch 192/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0165 - accuracy: 0.4536\n",
      "Epoch 193/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0169 - accuracy: 0.4537\n",
      "Epoch 194/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0163 - accuracy: 0.4538\n",
      "Epoch 195/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0169 - accuracy: 0.4535\n",
      "Epoch 196/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0165 - accuracy: 0.4537\n",
      "Epoch 197/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0168 - accuracy: 0.4537\n",
      "Epoch 198/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0163 - accuracy: 0.4539\n",
      "Epoch 199/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0163 - accuracy: 0.4536\n",
      "Epoch 200/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0161 - accuracy: 0.4538\n",
      "Epoch 201/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0162 - accuracy: 0.4538\n",
      "Epoch 202/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0163 - accuracy: 0.4539\n",
      "Epoch 203/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0165 - accuracy: 0.4537\n",
      "Epoch 204/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0162 - accuracy: 0.4540\n",
      "Epoch 205/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0163 - accuracy: 0.4538\n",
      "Epoch 206/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0159 - accuracy: 0.4538\n",
      "Epoch 207/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0165 - accuracy: 0.4537\n",
      "Epoch 208/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0162 - accuracy: 0.4538\n",
      "Epoch 209/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0163 - accuracy: 0.4538\n",
      "Epoch 210/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0161 - accuracy: 0.4538\n",
      "Epoch 211/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0160 - accuracy: 0.4538\n",
      "Epoch 212/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0164 - accuracy: 0.4538\n",
      "Epoch 213/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0160 - accuracy: 0.4538\n",
      "Epoch 214/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0159 - accuracy: 0.4539\n",
      "Epoch 215/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0159 - accuracy: 0.4539\n",
      "Epoch 216/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0159 - accuracy: 0.4536\n",
      "Epoch 217/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0159 - accuracy: 0.4539\n",
      "Epoch 218/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0158 - accuracy: 0.4539\n",
      "Epoch 219/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0158 - accuracy: 0.4539\n",
      "Epoch 220/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0158 - accuracy: 0.4540\n",
      "Epoch 221/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0158 - accuracy: 0.4537\n",
      "Epoch 222/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0157 - accuracy: 0.4539\n",
      "Epoch 223/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0160 - accuracy: 0.4538\n",
      "Epoch 224/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0154 - accuracy: 0.4539\n",
      "Epoch 225/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0159 - accuracy: 0.4539\n",
      "Epoch 226/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0158 - accuracy: 0.4540\n",
      "Epoch 227/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0156 - accuracy: 0.4539\n",
      "Epoch 228/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0157 - accuracy: 0.4540\n",
      "Epoch 229/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0153 - accuracy: 0.4540\n",
      "Epoch 230/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0155 - accuracy: 0.4540\n",
      "Epoch 231/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0152 - accuracy: 0.4540\n",
      "Epoch 232/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0151 - accuracy: 0.4541\n",
      "Epoch 233/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0156 - accuracy: 0.4539\n",
      "Epoch 234/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0153 - accuracy: 0.4539\n",
      "Epoch 235/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0155 - accuracy: 0.4539\n",
      "Epoch 236/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0151 - accuracy: 0.4540\n",
      "Epoch 237/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0152 - accuracy: 0.4540\n",
      "Epoch 238/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0157 - accuracy: 0.4540\n",
      "Epoch 239/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0152 - accuracy: 0.4540\n",
      "Epoch 240/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0153 - accuracy: 0.4540\n",
      "Epoch 241/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0151 - accuracy: 0.4541\n",
      "Epoch 242/1000\n",
      "117/117 [==============================] - 13s 108ms/step - loss: 0.0154 - accuracy: 0.4540\n",
      "Epoch 243/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0149 - accuracy: 0.4541\n",
      "Epoch 244/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0152 - accuracy: 0.4539\n",
      "Epoch 245/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0153 - accuracy: 0.4540\n",
      "Epoch 246/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0149 - accuracy: 0.4541\n",
      "Epoch 247/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0148 - accuracy: 0.4542\n",
      "Epoch 248/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0150 - accuracy: 0.4540\n",
      "Epoch 249/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0151 - accuracy: 0.4539\n",
      "Epoch 250/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0149 - accuracy: 0.4540\n",
      "Epoch 251/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0151 - accuracy: 0.4540\n",
      "Epoch 252/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0147 - accuracy: 0.4541\n",
      "Epoch 253/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0151 - accuracy: 0.4541\n",
      "Epoch 254/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0149 - accuracy: 0.4540\n",
      "Epoch 255/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0148 - accuracy: 0.4540\n",
      "Epoch 256/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0150 - accuracy: 0.4541\n",
      "Epoch 257/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0148 - accuracy: 0.4541\n",
      "Epoch 258/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0150 - accuracy: 0.4540\n",
      "Epoch 259/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0147 - accuracy: 0.4542\n",
      "Epoch 260/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0146 - accuracy: 0.4541\n",
      "Epoch 261/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0148 - accuracy: 0.4542\n",
      "Epoch 262/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0151 - accuracy: 0.4540\n",
      "Epoch 263/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0148 - accuracy: 0.4542\n",
      "Epoch 264/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0145 - accuracy: 0.4541\n",
      "Epoch 265/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0147 - accuracy: 0.4541\n",
      "Epoch 266/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0149 - accuracy: 0.4540\n",
      "Epoch 267/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0149 - accuracy: 0.4541\n",
      "Epoch 268/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0146 - accuracy: 0.4541\n",
      "Epoch 269/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0146 - accuracy: 0.4544\n",
      "Epoch 270/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0146 - accuracy: 0.4541\n",
      "Epoch 271/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0145 - accuracy: 0.4542\n",
      "Epoch 272/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0143 - accuracy: 0.4542\n",
      "Epoch 273/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0145 - accuracy: 0.4542\n",
      "Epoch 274/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0146 - accuracy: 0.4541\n",
      "Epoch 275/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0146 - accuracy: 0.4542\n",
      "Epoch 276/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0148 - accuracy: 0.4542\n",
      "Epoch 277/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0146 - accuracy: 0.4541\n",
      "Epoch 278/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0144 - accuracy: 0.4541\n",
      "Epoch 279/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0147 - accuracy: 0.4541\n",
      "Epoch 280/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0146 - accuracy: 0.4540\n",
      "Epoch 281/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0142 - accuracy: 0.4542\n",
      "Epoch 282/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0148 - accuracy: 0.4539\n",
      "Epoch 283/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0143 - accuracy: 0.4544\n",
      "Epoch 284/1000\n",
      "117/117 [==============================] - 13s 112ms/step - loss: 0.0144 - accuracy: 0.4541\n",
      "Epoch 285/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0143 - accuracy: 0.4543\n",
      "Epoch 286/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0144 - accuracy: 0.4541\n",
      "Epoch 287/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0144 - accuracy: 0.4542\n",
      "Epoch 288/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0145 - accuracy: 0.4541\n",
      "Epoch 289/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0143 - accuracy: 0.4542\n",
      "Epoch 290/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0142 - accuracy: 0.4542\n",
      "Epoch 291/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0142 - accuracy: 0.4542\n",
      "Epoch 292/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0144 - accuracy: 0.4542\n",
      "Epoch 293/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0142 - accuracy: 0.4541\n",
      "Epoch 294/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0145 - accuracy: 0.4542\n",
      "Epoch 295/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0144 - accuracy: 0.4542\n",
      "Epoch 296/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0145 - accuracy: 0.4542\n",
      "Epoch 297/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0144 - accuracy: 0.4541\n",
      "Epoch 298/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0143 - accuracy: 0.4541\n",
      "Epoch 299/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0145 - accuracy: 0.4541\n",
      "Epoch 300/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0140 - accuracy: 0.4543\n",
      "Epoch 301/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0141 - accuracy: 0.4542\n",
      "Epoch 302/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0145 - accuracy: 0.4542\n",
      "Epoch 303/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0140 - accuracy: 0.4543\n",
      "Epoch 304/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0142 - accuracy: 0.4544\n",
      "Epoch 305/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0141 - accuracy: 0.4541\n",
      "Epoch 306/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0142 - accuracy: 0.4543\n",
      "Epoch 307/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0140 - accuracy: 0.4544\n",
      "Epoch 308/1000\n",
      "117/117 [==============================] - 13s 115ms/step - loss: 0.0139 - accuracy: 0.4544\n",
      "Epoch 309/1000\n",
      "117/117 [==============================] - 14s 121ms/step - loss: 0.0141 - accuracy: 0.4542\n",
      "Epoch 310/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0140 - accuracy: 0.4541\n",
      "Epoch 311/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0142 - accuracy: 0.4544\n",
      "Epoch 312/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0141 - accuracy: 0.4542\n",
      "Epoch 313/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0139 - accuracy: 0.4541\n",
      "Epoch 314/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0141 - accuracy: 0.4542\n",
      "Epoch 315/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0139 - accuracy: 0.4541\n",
      "Epoch 316/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0141 - accuracy: 0.4543\n",
      "Epoch 317/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0144 - accuracy: 0.4542\n",
      "Epoch 318/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0140 - accuracy: 0.4541\n",
      "Epoch 319/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0141 - accuracy: 0.4542\n",
      "Epoch 320/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0139 - accuracy: 0.4543\n",
      "Epoch 321/1000\n",
      "117/117 [==============================] - 13s 112ms/step - loss: 0.0136 - accuracy: 0.4544\n",
      "Epoch 322/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0141 - accuracy: 0.4542\n",
      "Epoch 323/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0140 - accuracy: 0.4543\n",
      "Epoch 324/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0141 - accuracy: 0.4543\n",
      "Epoch 325/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0139 - accuracy: 0.4542\n",
      "Epoch 326/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0138 - accuracy: 0.4542\n",
      "Epoch 327/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0139 - accuracy: 0.4544\n",
      "Epoch 328/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0139 - accuracy: 0.4542\n",
      "Epoch 329/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0137 - accuracy: 0.4544\n",
      "Epoch 330/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0136 - accuracy: 0.4543\n",
      "Epoch 331/1000\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0137 - accuracy: 0.4543\n",
      "Epoch 332/1000\n",
      "117/117 [==============================] - 14s 121ms/step - loss: 0.0138 - accuracy: 0.4543\n",
      "Epoch 333/1000\n",
      "117/117 [==============================] - 15s 127ms/step - loss: 0.0139 - accuracy: 0.4541\n",
      "Epoch 334/1000\n",
      "117/117 [==============================] - 15s 125ms/step - loss: 0.0137 - accuracy: 0.4544\n",
      "Epoch 335/1000\n",
      "117/117 [==============================] - 14s 120ms/step - loss: 0.0137 - accuracy: 0.4542\n",
      "Epoch 336/1000\n",
      "117/117 [==============================] - 14s 117ms/step - loss: 0.0136 - accuracy: 0.4544\n",
      "Epoch 337/1000\n",
      "117/117 [==============================] - 14s 122ms/step - loss: 0.0136 - accuracy: 0.4544\n",
      "Epoch 338/1000\n",
      "117/117 [==============================] - 15s 126ms/step - loss: 0.0139 - accuracy: 0.4543\n",
      "Epoch 339/1000\n",
      "117/117 [==============================] - 18s 153ms/step - loss: 0.0138 - accuracy: 0.4542\n",
      "Epoch 340/1000\n",
      "117/117 [==============================] - 17s 147ms/step - loss: 0.0140 - accuracy: 0.4544\n",
      "Epoch 341/1000\n",
      "117/117 [==============================] - 16s 139ms/step - loss: 0.0137 - accuracy: 0.4543\n",
      "Epoch 342/1000\n",
      "117/117 [==============================] - 15s 124ms/step - loss: 0.0138 - accuracy: 0.4543\n",
      "Epoch 343/1000\n",
      "117/117 [==============================] - 15s 125ms/step - loss: 0.0139 - accuracy: 0.4542\n",
      "Epoch 344/1000\n",
      "117/117 [==============================] - 14s 122ms/step - loss: 0.0137 - accuracy: 0.4543\n",
      "Epoch 345/1000\n",
      "117/117 [==============================] - 14s 118ms/step - loss: 0.0138 - accuracy: 0.4544\n",
      "Epoch 346/1000\n",
      "117/117 [==============================] - 13s 115ms/step - loss: 0.0136 - accuracy: 0.4542\n",
      "Epoch 347/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0137 - accuracy: 0.4544\n",
      "Epoch 348/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0138 - accuracy: 0.4543\n",
      "Epoch 349/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0136 - accuracy: 0.4543\n",
      "Epoch 350/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0134 - accuracy: 0.4544\n",
      "Epoch 351/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0136 - accuracy: 0.4544\n",
      "Epoch 352/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0134 - accuracy: 0.4545\n",
      "Epoch 353/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0136 - accuracy: 0.4544\n",
      "Epoch 354/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0134 - accuracy: 0.4543\n",
      "Epoch 355/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0135 - accuracy: 0.4544\n",
      "Epoch 356/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0138 - accuracy: 0.4544\n",
      "Epoch 357/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0137 - accuracy: 0.4543\n",
      "Epoch 358/1000\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0134 - accuracy: 0.4546\n",
      "Epoch 359/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0136 - accuracy: 0.4543\n",
      "Epoch 360/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0135 - accuracy: 0.4544\n",
      "Epoch 361/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0135 - accuracy: 0.4544\n",
      "Epoch 362/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0135 - accuracy: 0.4544\n",
      "Epoch 363/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0136 - accuracy: 0.4543\n",
      "Epoch 364/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0136 - accuracy: 0.4543\n",
      "Epoch 365/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0134 - accuracy: 0.4544\n",
      "Epoch 366/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0136 - accuracy: 0.4544\n",
      "Epoch 367/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0134 - accuracy: 0.4545\n",
      "Epoch 368/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0135 - accuracy: 0.4543\n",
      "Epoch 369/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0138 - accuracy: 0.4543\n",
      "Epoch 370/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0136 - accuracy: 0.4543\n",
      "Epoch 371/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0134 - accuracy: 0.4543\n",
      "Epoch 372/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0134 - accuracy: 0.4544\n",
      "Epoch 373/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0133 - accuracy: 0.4545\n",
      "Epoch 374/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0137 - accuracy: 0.4543\n",
      "Epoch 375/1000\n",
      "117/117 [==============================] - 13s 114ms/step - loss: 0.0134 - accuracy: 0.4543\n",
      "Epoch 376/1000\n",
      "117/117 [==============================] - 13s 112ms/step - loss: 0.0132 - accuracy: 0.4545\n",
      "Epoch 377/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0133 - accuracy: 0.4544\n",
      "Epoch 378/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0133 - accuracy: 0.4543\n",
      "Epoch 379/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0134 - accuracy: 0.4544\n",
      "Epoch 380/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0133 - accuracy: 0.4545\n",
      "Epoch 381/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0133 - accuracy: 0.4544\n",
      "Epoch 382/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0133 - accuracy: 0.4543\n",
      "Epoch 383/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0134 - accuracy: 0.4544\n",
      "Epoch 384/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0134 - accuracy: 0.4543\n",
      "Epoch 385/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0135 - accuracy: 0.4544\n",
      "Epoch 386/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0129 - accuracy: 0.4544\n",
      "Epoch 387/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0131 - accuracy: 0.4546\n",
      "Epoch 388/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0134 - accuracy: 0.4544\n",
      "Epoch 389/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0131 - accuracy: 0.4544\n",
      "Epoch 390/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0133 - accuracy: 0.4544\n",
      "Epoch 391/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0133 - accuracy: 0.4544\n",
      "Epoch 392/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0132 - accuracy: 0.4544\n",
      "Epoch 393/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0131 - accuracy: 0.4546\n",
      "Epoch 394/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0137 - accuracy: 0.4543\n",
      "Epoch 395/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0132 - accuracy: 0.4544\n",
      "Epoch 396/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0131 - accuracy: 0.4545\n",
      "Epoch 397/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0132 - accuracy: 0.4544\n",
      "Epoch 398/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0131 - accuracy: 0.4545\n",
      "Epoch 399/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0131 - accuracy: 0.4545\n",
      "Epoch 400/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0133 - accuracy: 0.4543\n",
      "Epoch 401/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0131 - accuracy: 0.4544\n",
      "Epoch 402/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0131 - accuracy: 0.4545\n",
      "Epoch 403/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0132 - accuracy: 0.4543\n",
      "Epoch 404/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0131 - accuracy: 0.4544\n",
      "Epoch 405/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0133 - accuracy: 0.4544\n",
      "Epoch 406/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0133 - accuracy: 0.4544\n",
      "Epoch 407/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0133 - accuracy: 0.4544\n",
      "Epoch 408/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0132 - accuracy: 0.4544\n",
      "Epoch 409/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0131 - accuracy: 0.4545\n",
      "Epoch 410/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0132 - accuracy: 0.4544\n",
      "Epoch 411/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0132 - accuracy: 0.4545\n",
      "Epoch 412/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0129 - accuracy: 0.4544\n",
      "Epoch 413/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0131 - accuracy: 0.4544\n",
      "Epoch 414/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0133 - accuracy: 0.4544\n",
      "Epoch 415/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0131 - accuracy: 0.4545\n",
      "Epoch 416/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0132 - accuracy: 0.4544\n",
      "Epoch 417/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0132 - accuracy: 0.4544\n",
      "Epoch 418/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0130 - accuracy: 0.4546\n",
      "Epoch 419/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0132 - accuracy: 0.4545\n",
      "Epoch 420/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0129 - accuracy: 0.4545\n",
      "Epoch 421/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0132 - accuracy: 0.4543\n",
      "Epoch 422/1000\n",
      "117/117 [==============================] - 13s 112ms/step - loss: 0.0129 - accuracy: 0.4546\n",
      "Epoch 423/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0131 - accuracy: 0.4545\n",
      "Epoch 424/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0130 - accuracy: 0.4544\n",
      "Epoch 425/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0131 - accuracy: 0.4546\n",
      "Epoch 426/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0135 - accuracy: 0.4543\n",
      "Epoch 427/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0131 - accuracy: 0.4545\n",
      "Epoch 428/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0129 - accuracy: 0.4546\n",
      "Epoch 429/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0129 - accuracy: 0.4544\n",
      "Epoch 430/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0130 - accuracy: 0.4546\n",
      "Epoch 431/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0130 - accuracy: 0.4544\n",
      "Epoch 432/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0129 - accuracy: 0.4546\n",
      "Epoch 433/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0131 - accuracy: 0.4543\n",
      "Epoch 434/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0131 - accuracy: 0.4545\n",
      "Epoch 435/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0130 - accuracy: 0.4546\n",
      "Epoch 436/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0130 - accuracy: 0.4545\n",
      "Epoch 437/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0130 - accuracy: 0.4544\n",
      "Epoch 438/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0132 - accuracy: 0.4545\n",
      "Epoch 439/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0128 - accuracy: 0.4545\n",
      "Epoch 440/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0131 - accuracy: 0.4545\n",
      "Epoch 441/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0130 - accuracy: 0.4546\n",
      "Epoch 442/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0129 - accuracy: 0.4545\n",
      "Epoch 443/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0128 - accuracy: 0.4547\n",
      "Epoch 444/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0131 - accuracy: 0.4544\n",
      "Epoch 445/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0130 - accuracy: 0.4546\n",
      "Epoch 446/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0128 - accuracy: 0.4546\n",
      "Epoch 447/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0130 - accuracy: 0.4546\n",
      "Epoch 448/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0132 - accuracy: 0.4545\n",
      "Epoch 449/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0129 - accuracy: 0.4545\n",
      "Epoch 450/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0127 - accuracy: 0.4545\n",
      "Epoch 451/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0131 - accuracy: 0.4545\n",
      "Epoch 452/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0128 - accuracy: 0.4546\n",
      "Epoch 453/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0128 - accuracy: 0.4545\n",
      "Epoch 454/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0129 - accuracy: 0.4546\n",
      "Epoch 455/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0126 - accuracy: 0.4547\n",
      "Epoch 456/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0127 - accuracy: 0.4546\n",
      "Epoch 457/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0131 - accuracy: 0.4544\n",
      "Epoch 458/1000\n",
      "117/117 [==============================] - 13s 114ms/step - loss: 0.0128 - accuracy: 0.4546\n",
      "Epoch 459/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0127 - accuracy: 0.4545\n",
      "Epoch 460/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0129 - accuracy: 0.4545\n",
      "Epoch 461/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0128 - accuracy: 0.4545\n",
      "Epoch 462/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0131 - accuracy: 0.4545\n",
      "Epoch 463/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0128 - accuracy: 0.4546\n",
      "Epoch 464/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0129 - accuracy: 0.4545\n",
      "Epoch 465/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0129 - accuracy: 0.4546\n",
      "Epoch 466/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0128 - accuracy: 0.4545\n",
      "Epoch 467/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0129 - accuracy: 0.4546\n",
      "Epoch 468/1000\n",
      "117/117 [==============================] - 13s 112ms/step - loss: 0.0128 - accuracy: 0.4546\n",
      "Epoch 469/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0127 - accuracy: 0.4546\n",
      "Epoch 470/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0126 - accuracy: 0.4545\n",
      "Epoch 471/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0129 - accuracy: 0.4545\n",
      "Epoch 472/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0130 - accuracy: 0.4544\n",
      "Epoch 473/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0127 - accuracy: 0.4545\n",
      "Epoch 474/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0127 - accuracy: 0.4546\n",
      "Epoch 475/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0126 - accuracy: 0.4547\n",
      "Epoch 476/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0127 - accuracy: 0.4545\n",
      "Epoch 477/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0127 - accuracy: 0.4547\n",
      "Epoch 478/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0125 - accuracy: 0.4546\n",
      "Epoch 479/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0128 - accuracy: 0.4544\n",
      "Epoch 480/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0127 - accuracy: 0.4546\n",
      "Epoch 481/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0128 - accuracy: 0.4545\n",
      "Epoch 482/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0129 - accuracy: 0.4546\n",
      "Epoch 483/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0128 - accuracy: 0.4546\n",
      "Epoch 484/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0126 - accuracy: 0.4545\n",
      "Epoch 485/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0129 - accuracy: 0.4545\n",
      "Epoch 486/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0127 - accuracy: 0.4546\n",
      "Epoch 487/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0126 - accuracy: 0.4548\n",
      "Epoch 488/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0130 - accuracy: 0.4545\n",
      "Epoch 489/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0128 - accuracy: 0.4546\n",
      "Epoch 490/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0126 - accuracy: 0.4546\n",
      "Epoch 491/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0127 - accuracy: 0.4546\n",
      "Epoch 492/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0128 - accuracy: 0.4546\n",
      "Epoch 493/1000\n",
      "117/117 [==============================] - 14s 116ms/step - loss: 0.0126 - accuracy: 0.4546\n",
      "Epoch 494/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0126 - accuracy: 0.4546\n",
      "Epoch 495/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0126 - accuracy: 0.4546\n",
      "Epoch 496/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0126 - accuracy: 0.4547\n",
      "Epoch 497/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0127 - accuracy: 0.4548\n",
      "Epoch 498/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0127 - accuracy: 0.4546\n",
      "Epoch 499/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0131 - accuracy: 0.4544\n",
      "Epoch 500/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0128 - accuracy: 0.4547\n",
      "Epoch 501/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0128 - accuracy: 0.4545\n",
      "Epoch 502/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0126 - accuracy: 0.4546\n",
      "Epoch 503/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0127 - accuracy: 0.4545\n",
      "Epoch 504/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4547\n",
      "Epoch 505/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0128 - accuracy: 0.4546\n",
      "Epoch 506/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0127 - accuracy: 0.4545\n",
      "Epoch 507/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4547\n",
      "Epoch 508/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0126 - accuracy: 0.4546\n",
      "Epoch 509/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4546\n",
      "Epoch 510/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4547\n",
      "Epoch 511/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4546\n",
      "Epoch 512/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0126 - accuracy: 0.4547\n",
      "Epoch 513/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4545\n",
      "Epoch 514/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0126 - accuracy: 0.4547\n",
      "Epoch 515/1000\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0127 - accuracy: 0.4546\n",
      "Epoch 516/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4546\n",
      "Epoch 517/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0127 - accuracy: 0.4545\n",
      "Epoch 518/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0127 - accuracy: 0.4546\n",
      "Epoch 519/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0126 - accuracy: 0.4545\n",
      "Epoch 520/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0126 - accuracy: 0.4547\n",
      "Epoch 521/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0127 - accuracy: 0.4546\n",
      "Epoch 522/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0127 - accuracy: 0.4545\n",
      "Epoch 523/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0125 - accuracy: 0.4545\n",
      "Epoch 524/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4547\n",
      "Epoch 525/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0128 - accuracy: 0.4545\n",
      "Epoch 526/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0127 - accuracy: 0.4545\n",
      "Epoch 527/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0126 - accuracy: 0.4547\n",
      "Epoch 528/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4545\n",
      "Epoch 529/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0124 - accuracy: 0.4546\n",
      "Epoch 530/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0124 - accuracy: 0.4546\n",
      "Epoch 531/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4546\n",
      "Epoch 532/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4547\n",
      "Epoch 533/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0126 - accuracy: 0.4546\n",
      "Epoch 534/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0126 - accuracy: 0.4546\n",
      "Epoch 535/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4547\n",
      "Epoch 536/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0125 - accuracy: 0.4546\n",
      "Epoch 537/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4546\n",
      "Epoch 538/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0124 - accuracy: 0.4547\n",
      "Epoch 539/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0125 - accuracy: 0.4546\n",
      "Epoch 540/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0124 - accuracy: 0.4546\n",
      "Epoch 541/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4547\n",
      "Epoch 542/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4547\n",
      "Epoch 543/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0126 - accuracy: 0.4546\n",
      "Epoch 544/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4546\n",
      "Epoch 545/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0123 - accuracy: 0.4549\n",
      "Epoch 546/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0127 - accuracy: 0.4545\n",
      "Epoch 547/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0123 - accuracy: 0.4547\n",
      "Epoch 548/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4546\n",
      "Epoch 549/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0126 - accuracy: 0.4547\n",
      "Epoch 550/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0124 - accuracy: 0.4546\n",
      "Epoch 551/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0127 - accuracy: 0.4545\n",
      "Epoch 552/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4547\n",
      "Epoch 553/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4546\n",
      "Epoch 554/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0126 - accuracy: 0.4546\n",
      "Epoch 555/1000\n",
      "117/117 [==============================] - 14s 117ms/step - loss: 0.0125 - accuracy: 0.4546\n",
      "Epoch 556/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4548\n",
      "Epoch 557/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 558/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4547\n",
      "Epoch 559/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 560/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4547\n",
      "Epoch 561/1000\n",
      "117/117 [==============================] - 13s 112ms/step - loss: 0.0125 - accuracy: 0.4546\n",
      "Epoch 562/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0124 - accuracy: 0.4547\n",
      "Epoch 563/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0124 - accuracy: 0.4547\n",
      "Epoch 564/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0124 - accuracy: 0.4547\n",
      "Epoch 565/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4547\n",
      "Epoch 566/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4549\n",
      "Epoch 567/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0124 - accuracy: 0.4545\n",
      "Epoch 568/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0124 - accuracy: 0.4546\n",
      "Epoch 569/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4546\n",
      "Epoch 570/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4548\n",
      "Epoch 571/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0126 - accuracy: 0.4547\n",
      "Epoch 572/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0124 - accuracy: 0.4547\n",
      "Epoch 573/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0123 - accuracy: 0.4546\n",
      "Epoch 574/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0124 - accuracy: 0.4547\n",
      "Epoch 575/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0123 - accuracy: 0.4547\n",
      "Epoch 576/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0124 - accuracy: 0.4546\n",
      "Epoch 577/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0126 - accuracy: 0.4545\n",
      "Epoch 578/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4546\n",
      "Epoch 579/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0124 - accuracy: 0.4547\n",
      "Epoch 580/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4546\n",
      "Epoch 581/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0124 - accuracy: 0.4546\n",
      "Epoch 582/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0124 - accuracy: 0.4546\n",
      "Epoch 583/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0123 - accuracy: 0.4547\n",
      "Epoch 584/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0124 - accuracy: 0.4547\n",
      "Epoch 585/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4547\n",
      "Epoch 586/1000\n",
      "117/117 [==============================] - 13s 112ms/step - loss: 0.0124 - accuracy: 0.4547\n",
      "Epoch 587/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 588/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0124 - accuracy: 0.4547\n",
      "Epoch 589/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0124 - accuracy: 0.4546\n",
      "Epoch 590/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0123 - accuracy: 0.4548\n",
      "Epoch 591/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0126 - accuracy: 0.4546\n",
      "Epoch 592/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0124 - accuracy: 0.4546\n",
      "Epoch 593/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0124 - accuracy: 0.4546\n",
      "Epoch 594/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0125 - accuracy: 0.4545\n",
      "Epoch 595/1000\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0124 - accuracy: 0.4547\n",
      "Epoch 596/1000\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0123 - accuracy: 0.4546\n",
      "Epoch 597/1000\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0124 - accuracy: 0.4547\n",
      "Epoch 598/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0125 - accuracy: 0.4546\n",
      "Epoch 599/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0123 - accuracy: 0.4549\n",
      "Epoch 600/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0124 - accuracy: 0.4545\n",
      "Epoch 601/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0124 - accuracy: 0.4546\n",
      "Epoch 602/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 603/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4548\n",
      "Epoch 604/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0123 - accuracy: 0.4547\n",
      "Epoch 605/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0123 - accuracy: 0.4546\n",
      "Epoch 606/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4546\n",
      "Epoch 607/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0123 - accuracy: 0.4546\n",
      "Epoch 608/1000\n",
      "117/117 [==============================] - 13s 112ms/step - loss: 0.0123 - accuracy: 0.4548\n",
      "Epoch 609/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0123 - accuracy: 0.4547\n",
      "Epoch 610/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0123 - accuracy: 0.4546\n",
      "Epoch 611/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0125 - accuracy: 0.4546\n",
      "Epoch 612/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0123 - accuracy: 0.4547\n",
      "Epoch 613/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0125 - accuracy: 0.4547\n",
      "Epoch 614/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0125 - accuracy: 0.4547\n",
      "Epoch 615/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0123 - accuracy: 0.4547\n",
      "Epoch 616/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0124 - accuracy: 0.4547\n",
      "Epoch 617/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0123 - accuracy: 0.4546\n",
      "Epoch 618/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0123 - accuracy: 0.4547\n",
      "Epoch 619/1000\n",
      "117/117 [==============================] - 13s 112ms/step - loss: 0.0124 - accuracy: 0.4548\n",
      "Epoch 620/1000\n",
      "117/117 [==============================] - 14s 118ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 621/1000\n",
      "117/117 [==============================] - 14s 116ms/step - loss: 0.0122 - accuracy: 0.4548\n",
      "Epoch 622/1000\n",
      "117/117 [==============================] - 13s 108ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 623/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 624/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 625/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0122 - accuracy: 0.4548\n",
      "Epoch 626/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 627/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0123 - accuracy: 0.4546\n",
      "Epoch 628/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0123 - accuracy: 0.4548\n",
      "Epoch 629/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0124 - accuracy: 0.4547\n",
      "Epoch 630/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 631/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 632/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 633/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 634/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4547\n",
      "Epoch 635/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0123 - accuracy: 0.4547\n",
      "Epoch 636/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4548\n",
      "Epoch 637/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0123 - accuracy: 0.4547\n",
      "Epoch 638/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0123 - accuracy: 0.4546\n",
      "Epoch 639/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 640/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 641/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 642/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 643/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 644/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 645/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0123 - accuracy: 0.4547\n",
      "Epoch 646/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4549\n",
      "Epoch 647/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0123 - accuracy: 0.4547\n",
      "Epoch 648/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4548\n",
      "Epoch 649/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0123 - accuracy: 0.4547\n",
      "Epoch 650/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4547\n",
      "Epoch 651/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4546\n",
      "Epoch 652/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 653/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4546\n",
      "Epoch 654/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0121 - accuracy: 0.4547\n",
      "Epoch 655/1000\n",
      "117/117 [==============================] - 14s 121ms/step - loss: 0.0122 - accuracy: 0.4546\n",
      "Epoch 656/1000\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0121 - accuracy: 0.4547\n",
      "Epoch 657/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0123 - accuracy: 0.4547\n",
      "Epoch 658/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 659/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0122 - accuracy: 0.4548\n",
      "Epoch 660/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4546\n",
      "Epoch 661/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0119 - accuracy: 0.4547\n",
      "Epoch 662/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 663/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 664/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4548\n",
      "Epoch 665/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0121 - accuracy: 0.4547\n",
      "Epoch 666/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0119 - accuracy: 0.4550\n",
      "Epoch 667/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0121 - accuracy: 0.4546\n",
      "Epoch 668/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4547\n",
      "Epoch 669/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 670/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0124 - accuracy: 0.4548\n",
      "Epoch 671/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 672/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4547\n",
      "Epoch 673/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0123 - accuracy: 0.4547\n",
      "Epoch 674/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4549\n",
      "Epoch 675/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 676/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 677/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 678/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4547\n",
      "Epoch 679/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4549\n",
      "Epoch 680/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0121 - accuracy: 0.4547\n",
      "Epoch 681/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4547\n",
      "Epoch 682/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4548\n",
      "Epoch 683/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0123 - accuracy: 0.4548\n",
      "Epoch 684/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 685/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0119 - accuracy: 0.4548\n",
      "Epoch 686/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0122 - accuracy: 0.4548\n",
      "Epoch 687/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 688/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4547\n",
      "Epoch 689/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 690/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4548\n",
      "Epoch 691/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4549\n",
      "Epoch 692/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 693/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0123 - accuracy: 0.4548\n",
      "Epoch 694/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4547\n",
      "Epoch 695/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 696/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 697/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 698/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4547\n",
      "Epoch 699/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 700/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 701/1000\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 702/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4547\n",
      "Epoch 703/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 704/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 705/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 706/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 707/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4546\n",
      "Epoch 708/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 709/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4546\n",
      "Epoch 710/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4547\n",
      "Epoch 711/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4547\n",
      "Epoch 712/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0124 - accuracy: 0.4547\n",
      "Epoch 713/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 714/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 715/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 716/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0119 - accuracy: 0.4548\n",
      "Epoch 717/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4547\n",
      "Epoch 718/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0119 - accuracy: 0.4548\n",
      "Epoch 719/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 720/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4547\n",
      "Epoch 721/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 722/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4548\n",
      "Epoch 723/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 724/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 725/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0119 - accuracy: 0.4548\n",
      "Epoch 726/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4547\n",
      "Epoch 727/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0119 - accuracy: 0.4547\n",
      "Epoch 728/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 729/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0119 - accuracy: 0.4548\n",
      "Epoch 730/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4547\n",
      "Epoch 731/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4549\n",
      "Epoch 732/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0119 - accuracy: 0.4548\n",
      "Epoch 733/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4546\n",
      "Epoch 734/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 735/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0118 - accuracy: 0.4548\n",
      "Epoch 736/1000\n",
      "117/117 [==============================] - 13s 114ms/step - loss: 0.0119 - accuracy: 0.4548\n",
      "Epoch 737/1000\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 738/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0123 - accuracy: 0.4547\n",
      "Epoch 739/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 740/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0119 - accuracy: 0.4548\n",
      "Epoch 741/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0119 - accuracy: 0.4547\n",
      "Epoch 742/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4547\n",
      "Epoch 743/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4547\n",
      "Epoch 744/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 745/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0119 - accuracy: 0.4546\n",
      "Epoch 746/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0119 - accuracy: 0.4547\n",
      "Epoch 747/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4549\n",
      "Epoch 748/1000\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0118 - accuracy: 0.4548\n",
      "Epoch 749/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 750/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0121 - accuracy: 0.4547\n",
      "Epoch 751/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0120 - accuracy: 0.4547\n",
      "Epoch 752/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4546\n",
      "Epoch 753/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 754/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0119 - accuracy: 0.4548\n",
      "Epoch 755/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 756/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0118 - accuracy: 0.4550\n",
      "Epoch 757/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0122 - accuracy: 0.4548\n",
      "Epoch 758/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0119 - accuracy: 0.4548\n",
      "Epoch 759/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0120 - accuracy: 0.4549\n",
      "Epoch 760/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0122 - accuracy: 0.4548\n",
      "Epoch 761/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 762/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 763/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 764/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 765/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0119 - accuracy: 0.4547\n",
      "Epoch 766/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4547\n",
      "Epoch 767/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0118 - accuracy: 0.4548\n",
      "Epoch 768/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0119 - accuracy: 0.4548\n",
      "Epoch 769/1000\n",
      "117/117 [==============================] - 15s 125ms/step - loss: 0.0121 - accuracy: 0.4547\n",
      "Epoch 770/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0118 - accuracy: 0.4549\n",
      "Epoch 771/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0119 - accuracy: 0.4547\n",
      "Epoch 772/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4547\n",
      "Epoch 773/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 774/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4548\n",
      "Epoch 775/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0118 - accuracy: 0.4550\n",
      "Epoch 776/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0119 - accuracy: 0.4549\n",
      "Epoch 777/1000\n",
      "117/117 [==============================] - 13s 109ms/step - loss: 0.0119 - accuracy: 0.4548\n",
      "Epoch 778/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4549\n",
      "Epoch 779/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0119 - accuracy: 0.4549\n",
      "Epoch 780/1000\n",
      "117/117 [==============================] - 14s 120ms/step - loss: 0.0119 - accuracy: 0.4549\n",
      "Epoch 781/1000\n",
      "117/117 [==============================] - 13s 114ms/step - loss: 0.0119 - accuracy: 0.4548\n",
      "Epoch 782/1000\n",
      "117/117 [==============================] - 13s 112ms/step - loss: 0.0121 - accuracy: 0.4548\n",
      "Epoch 783/1000\n",
      "117/117 [==============================] - 13s 112ms/step - loss: 0.0118 - accuracy: 0.4548\n",
      "Epoch 784/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0119 - accuracy: 0.4548\n",
      "Epoch 785/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0119 - accuracy: 0.4548\n",
      "Epoch 786/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0119 - accuracy: 0.4548\n",
      "Epoch 787/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0120 - accuracy: 0.4549\n",
      "Epoch 788/1000\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0119 - accuracy: 0.4548\n",
      "Epoch 789/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0119 - accuracy: 0.4548\n",
      "Epoch 790/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0120 - accuracy: 0.4549\n",
      "Epoch 791/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0121 - accuracy: 0.4549\n",
      "Epoch 792/1000\n",
      "117/117 [==============================] - 13s 111ms/step - loss: 0.0119 - accuracy: 0.4549\n",
      "Epoch 793/1000\n",
      " 18/117 [===>..........................] - ETA: 11s - loss: 0.0116 - accuracy: 0.4665"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KIMDAE~1\\AppData\\Local\\Temp/ipykernel_18740/2276817538.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 1000\n",
    "model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "dOTNLOCdArDi"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 예측 시작\n",
    "  for i in range(MAX_LENGTH):\n",
    "    predictions = model(inputs=[sentence, output], training=False)\n",
    "\n",
    "    # 현재(마지막) 시점의 예측 단어를 받아온다.\n",
    "    predictions = predictions[:, -1:, :]\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 마지막 시점의 예측 단어를 출력에 연결한다.\n",
    "    # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0)\n",
    "\n",
    "\n",
    "def predict(sentence):\n",
    "  prediction = evaluate(sentence)\n",
    "\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "PAHvRASLBQyz"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "1IYld-HtmP2f"
   },
   "outputs": [],
   "source": [
    "# 딥러닝 model 객체 저장.\n",
    "\n",
    "model.save_weights('conversation_weight_data1111')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ujmHO_QdUTji",
    "outputId": "46f1fcfb-f2d6-4c09-fb6d-6e7e8e2fe49f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 무료로 검사해주는 사유는 무엇인가요?\n",
      "Output: [CLS] 질병의 확산 방지를 위해 적극적인 검사에 참여토록 하기 위함입니다.[SEP]\n"
     ]
    }
   ],
   "source": [
    "output = predict('무료로 검사해주는 사유는 무엇인가요?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3H--FxhUUoC",
    "outputId": "3bdbe8cf-9b71-497f-9558-06b0a090e662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 백신 부작용\n",
      "Output: [CLS] 네, 장기능 장애, 저혈압등의 부작용을 보였습니다.[SEP]\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"백신 부작용\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mW_bzLFCUWrg",
    "outputId": "d399f0f6-960a-41f3-af5a-aa6bf08446f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 코로나 백신 접종되나요?\n",
      "Output: [CLS] 네, 궁금하신 점이 있으신가요?[SEP]\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"코로나 백신 접종되나요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYqi9AeZUl_q",
    "outputId": "ea08d815-56d7-446a-e965-d2719baf87bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 코로나 검사를 받고 싶어요\n",
      "Output: [CLS] 가까운 보건소에서 받으시면 됩니다.[SEP]\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"코로나 검사를 받고 싶어요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O6GJLMO1UorF",
    "outputId": "0479a7a5-92cc-436c-f5f4-013450ca9836"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 열나요\n",
      "Output: [CLS] 일단, 증상이 심하시면 병원으로 보내야 해요.[SEP]\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"열나요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 마스크 착용해야 되요?\n",
      "Output: [CLS] 머리 끈을 아래로 늘어뜨리고 코와 턱이 감싸지도지도록 얼굴에 하세요.[SEP]\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"마스크 착용해야 되요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tUl6DCxFUvm2",
    "outputId": "97fa39fc-4ea5-489b-9578-367547950f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 치료약 있나요\n",
      "Output: [CLS] 네[SEP]\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"치료약 있나요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vc8K2Lm6yC4",
    "outputId": "833693bd-6156-421d-d1a9-82e2eac81aa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 자가격리 되면 아침은 어떻게 먹나요?\n",
      "Output: [CLS] 개별적으로 해결해야 합니다.[SEP]\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"자가격리 되면 아침은 어떻게 먹나요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I53p82_2Aqml",
    "outputId": "bfe29e19-6eb6-434a-999f-596275731980"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 백신은 언제 맞을까요?\n",
      "Output: [CLS] 현재 개발단계입니다.[SEP]\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"백신은 언제 맞을까요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2EsCRhRAxx8",
    "outputId": "adcafaa3-f019-4ea7-efbc-a9061f2a8e3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 39조 어케될까?\n",
      "Output: [CLS] 독감으로 인해 열이 나면 코로나 검사를 받아야 한다고 생각들 하셔서 그런것 같습니다.[SEP]\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"39조 어케될까?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q0HQrpllA5we",
    "outputId": "ef3389bd-3681-4b5b-c80b-25f57d0a0aa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 백신 부작용은?\n",
      "Output: [CLS] 네, 장기능 장애, 저혈압등의 부작용을 보였습니다.[SEP]\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"백신 부작용은?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "No2aSXowBCR6",
    "outputId": "e880f9f3-aaa2-482e-9c8c-8dd7ba8c5af0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zRyNaR0KCw-D",
    "outputId": "178e032a-1a0b-44b2-dbb3-a95a93fb83f2"
   },
   "outputs": [],
   "source": [
    "output = predict(\"오늘의 날씨는?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQasQWbiCzyY",
    "outputId": "8b038ad9-3aa1-4d34-cb9f-f854b96724a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 안녕하세요\n",
      "Output: [CLS] 안녕하세요[SEP]\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"안녕하세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glzAovWVs4wa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Transformer Chatbot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
